{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00aa5b45-14ed-4530-964d-862332f7bce2",
   "metadata": {},
   "source": [
    "# Creating a Cleaned Dataset for Snow Spotter (Water Year 2021)\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Zooniverse data exports  provide a massive csv file with a huge range of data. However, for data analysis purposes, we are only interested in a a few fields. This script describes the code used to create a cleaned netCDF or csv file for Niwot Ridge Water Year 2021. The final cleaned file will have columns for datetime, median value, mean value, and mean threshold value. Note that this code is specific to the 2021 Water Year. The Snow Spotter Data Cleaning repository contains jupyter notebook files for other water years. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432951be-2aa4-4276-ac54-d26472fae401",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### First to import the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec6b40c4-e6f6-40e6-9c19-92ec787b7bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a2867-a694-4e4e-920e-c29157264cbc",
   "metadata": {},
   "source": [
    "##### Now import the data export as a dataframe and have pandas read the \"subject_data\" and \"annotations\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24f344-8c92-439d-9d93-0c53c7631c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\\\JohnsWork\\\\NiwotRidge\\\\Niwot_WY2021\\\\NiwotWY2021_Classifications_Data.csv\", usecols = ['subject_data', 'annotations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9cfbf5-a80e-4147-afbb-8f66233f84f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Next, extract the datetime from the subject_data column.\n",
    "\n",
    " * First, the cell is transformed into a dict so that we can call the 'metadata' key. \n",
    " \n",
    " * If you label your metadata column appropriately, this will already be in the form Y_m_d_HMS. If you labeled your column in another way, you will have to adjust this value so that it only contains the date. \n",
    " \n",
    " * We can then use strip the datetime, finishing creating a column with just datetimes which we will need for our analysis later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d4e8048a-7040-44b7-a865-71019a6d0cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "row_count = len(df.index)\n",
    "def extract_meta_data(index):\n",
    "    subject_data = df.iloc[index,1]\n",
    "    #string to dict\n",
    "    subject_dict = json.loads(subject_data)\n",
    "    #shed outer layer of dict\n",
    "    shedded_subject_dict = dict(ele for sub in subject_dict.values() for ele in sub.items())\n",
    "    #extract the metadata, see important note in markdown above\n",
    "    metadata = shedded_subject_dict['Filename']\n",
    "    if len(metadata) == 28:\n",
    "        date = metadata[7:len(metadata)-4]\n",
    "    else:\n",
    "        date = metadata[7:len(metadata)-8]\n",
    "    #extract datetime, see important note in markdown above\n",
    "    pre_date_time = datetime.strptime(date, '%Y_%m_%d_%H%M%S')\n",
    "    if pre_date_time.month == 4:\n",
    "        date_time = pre_date_time + relativedelta(years=1)\n",
    "    else: date_time = pre_date_time\n",
    "    \n",
    "    return date_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5284def6-6587-4ece-9918-5cc11622faa5",
   "metadata": {},
   "source": [
    "#### The next function extracts the participant response\n",
    "\n",
    " * Once again, the cell must first be transformed into a dict so that we can call the 'value' key.\n",
    " \n",
    " * This value will be either \"Yes\", \"No\", or \"Unsure.\" We need to convert this into a numerical value so that we can average and graph responses later on. We use a simple if then loop to convert \"Yes\" to 1.0, \"No\" to 0.0, and \"Unsure\" to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "164f4d08-62e6-49b0-8f4a-286e1259676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value(index):\n",
    "    annotation = df.iloc[index,0]\n",
    "    #get rid of brackets \n",
    "    str_annotation = str(annotation)\n",
    "    annotation_dict_str = str_annotation[1:len(str_annotation)-1]\n",
    "    #string to dict\n",
    "    annotation_dict = json.loads(annotation_dict_str)\n",
    "    #grab yes or no value from dict \n",
    "    annotation_value = annotation_dict['value']\n",
    "\n",
    "    if(annotation_value == 'Yes'): \n",
    "        return 1\n",
    "    elif(annotation_value == 'No'): \n",
    "        return 0\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a126fa63-fb73-43e9-87ec-a5abb4d2a1f6",
   "metadata": {},
   "source": [
    "#### Now, we combine the extracted datetime and reponse into a new dataframe, and then find and combine the median and mean response for each datetime. \n",
    "\n",
    "The resulting dataframe will have one column for datetime, one column for median value, and one column for mean value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57bc212-39cf-4077-8afa-856f750ad27f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0       \n",
    "while (i < (len(df)-1)):\n",
    "    i = i + 1\n",
    "    data = []\n",
    "    data.append(extract_meta_data(i))\n",
    "    data.append(extract_value(i))\n",
    "    arr.append(data)\n",
    "    \n",
    "    \n",
    "final = pd.DataFrame(arr, columns = ['datetime','value'])\n",
    "\n",
    "median_final = final.groupby('datetime').median().reset_index()\n",
    "mean_final = final.groupby('datetime').mean().reset_index()\n",
    "combined_final = median_final\n",
    "combined_final['mean_value'] = mean_final.value\n",
    "combined_final.columns = ['datetime', 'median_value', 'mean_value']\n",
    "#create the \"mean_threshold\" column\n",
    "combined_final['mean_threshold'] = np.where(combined_final['mean_value'] >= 0.9, 1, np.where(np.isnan(combined_final['mean_value']), np.nan, 0))\n",
    "combined_final.set_index('datetime', inplace=True)\n",
    "combined_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae78a73-b80c-444f-97fb-35502e147fdf",
   "metadata": {},
   "source": [
    "#### Save the cleaned data set\n",
    "\n",
    "This code will save the cleaned dataset as a NetCDF file for easy management. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9a40f4b9-29c0-4deb-ba88-760b4d25013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_export = combined_final.to_xarray()\n",
    "cleaned_export.to_netcdf(\"D:\\\\JohnsWork\\\\NiwotRidge\\\\Cleaned Data\\\\netCDF\\\\NiwotWY2021_Cleaned_Data.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cb2364de-1bfb-44b0-bf42-b69daf7993ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_final.to_csv(\"D:\\\\JohnsWork\\\\NiwotRidge\\\\Cleaned Data\\\\csv\\\\NiwotWY2021_Cleaned_Data.csv\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
