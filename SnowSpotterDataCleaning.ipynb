{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5af7a8e-ca56-40d2-8859-0e6aaeedb3eb",
   "metadata": {},
   "source": [
    "# Creating a Cleaned Dataset for Snow Spotter (Generic)\n",
    "\n",
    "#### Summary\n",
    "\n",
    "Zooniverse data exports provide a massive csv file with a huge range of data. However, for data analysis purposes, we are only interested in a a few fields. This script describes the general approach to create a cleaned netCDF or csv file in python. The final cleaned file will have columns for datetime, median value, mean value, and mean threshold value. Note that this code is only a template. For specific code, the Snow Spotter Data Cleaning repository contains jupyter notebook files for cleaning data for water years 2016-2022 at Niwot Ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a3d4c-725a-49ff-8361-37b77f555f9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b92cf27-02c2-4ff7-963a-e03d1ed19247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217fdae7-1c0e-4fd0-8be3-a9b2820bb859",
   "metadata": {},
   "source": [
    "##### Import the data export as a dataframe with only the subject_data and annotations columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364db5d2-1ae7-43f7-8186-94a722db863d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"filepath\\\\filename.csv\", usecols = ['subject_data', 'annotations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77dc84-c877-4b72-82a1-b38786006b57",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The following code extracts the datetime from the subject_data column.\n",
    "\n",
    " * First, the cell is transformed into a dict so that we can call the 'metadata' key. \n",
    " \n",
    " * If the metadata column is labeled appropriately, this will already be in the form Y_m_d_HMS. If it was labeled in another way, the medatada value will have to be adjusted to contain only the date and time. \n",
    " \n",
    " * Strip the datetime, creating a column with just datetimes.\n",
    " \n",
    "**Important Note:** When uploading images to Zooniverse, the choice of medatata label will have an impact on the following code. It is best practice to label the metadata with only the image date and time, as this will limit the number of steps needed to extract a datetime from the classifications export. However, if something else is chosen, additional lines of code can extract the date from the metadata. While not preferable, as long as the date is somewhere within the metadata label, it can be extracted. For instance, if I had labeled my metadata with with both location and date, such as \"niwot3_2021_12_01_095605\", I would add this line of code to remove \"niwot3_\".  \n",
    "\n",
    "```python\n",
    "    date = metadata[7:len(metadata)]\n",
    "```\n",
    "Choice of metadata label can also affect datetime extraction, and the strip datetime code can be adjusted to fit any datetime format. The code below assumes the metadata is in the form Year_Month_Day_HourMinuteSecond (such as 2021_12_01_095605), but this can also be adjusted if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce126aa7-3dad-40fe-8bcf-8142267b24fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#row_count = len(df.index)\n",
    "def extract_meta_data(index):\n",
    "    subject_data = df.iloc[index,1]\n",
    "    #string to dict\n",
    "    subject_dict = json.loads(subject_data)\n",
    "    #shed outer layer of dict\n",
    "    shedded_subject_dict = dict(ele for sub in subject_dict.values() for ele in sub.items())\n",
    "    #extract the metadata, see important note in markdown above\n",
    "    metadata = shedded_subject_dict_simple['metadata']\n",
    "    #extract datetime, see important note in markdown above\n",
    "    date_time = datetime.strptime(date, '%Y_%m_%d_%H%M%S')\n",
    "    return date_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb13424-ce89-4d6c-8a96-5fdba0c017ea",
   "metadata": {},
   "source": [
    "#### The next function extracts the participant response\n",
    "\n",
    " * Transform the cell to a dict to call the 'value' key.\n",
    " \n",
    " * This value will be either \"Yes\", \"No\", or \"Unsure.\" To average and graph responses later on, these values are converted using a simple loop such that responses of \"Yes\" are recorded as 1.0, \"No\" are recorded as 0.0, and \"Unsure\" or \"it's dark\" are recorded as NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedbeed5-5822-4c6c-b0ac-9b6c9f7c2d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value(index):\n",
    "    annotation = df.iloc[index,0]\n",
    "    #convert to string\n",
    "    str_annotation = str(annotation)\n",
    "    #get rid of brackets around dict\n",
    "    annotation_dict_str = str_annotation[1:len(str_annotation)-1]\n",
    "    #string to dict\n",
    "    annotation_dict = json.loads(annotation_dict_str)\n",
    "    #grab yes or no value from dict \n",
    "    annotation_value = annotation_dict['value']\n",
    "\n",
    "    if(annotation_value == \"Yes\"): \n",
    "        return 1\n",
    "    elif(annotation_value == \"No\"): \n",
    "        return 0\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb7154-1ae5-4287-90e5-03d716f01b44",
   "metadata": {},
   "source": [
    "#### Combine Data and Calculate Statistics\n",
    "Use the above functions to create a new dataframe with the datetime and value for each participant response. Each unique image recieves 15 individual classifications, so responses must be averaged. Calculate the median and mean response value for each datetime and create a new dataframe using these values. The mean value is used to calculate snow presence based on a threshold in an additional column. If the mean value is greater than or equal to 0.9, the mean threshold column records a value of 1. This metric better captures dates when only significant amounts of snow are present in the canopy than the median. Finally, reindex by datetime. \n",
    "\n",
    "To summarize, the final dataframe will be indexed by datetime and have columns for median response, mean repsonse, and mean threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc081c99-6610-4a2d-9189-c43134eabd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arr = []\n",
    "i = 0       \n",
    "while (i < 65449):\n",
    "    i = i + 1\n",
    "    data = []\n",
    "    data.append(extract_meta_data(i))\n",
    "    data.append(extract_value(i))\n",
    "    arr.append(data)\n",
    "    \n",
    "    \n",
    "final = pd.DataFrame(arr, columns = ['datetime','value'])\n",
    "\n",
    "#average 15 participant responses\n",
    "median_final = final.groupby('datetime').median().reset_index()\n",
    "mean_final = final.groupby('datetime').mean().reset_index()\n",
    "combined_final = median_final\n",
    "combined_final['mean_value'] = mean_final.value\n",
    "combined_final.columns = ['datetime', 'median_value', 'mean_value']\n",
    "#add mean threshold column\n",
    "combined_final['mean_threshold'] = np.where(combined_final['mean_value'] >= 0.9, 1, np.where(np.isnan(combined_final['mean_value']), np.nan, 0))\n",
    "#reindex to datetime\n",
    "combined_final.set_index('datetime', inplace=True)\n",
    "combined_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab9a9d-21f0-4c6c-b0c1-bc03f24b7857",
   "metadata": {},
   "source": [
    "##### Save the cleaned data set as a netCDF file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f14401a4-b921-4056-b28c-c8630bccb2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_export = combined_final.to_xarray()\n",
    "cleaned_export.to_netcdf(\"filepath\\\\filename.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5531d603-39cb-4ca3-88ea-75ebb0872242",
   "metadata": {},
   "source": [
    "##### ... or as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c5d28-3a87-4d60-923f-a00df90a66b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_final.to_csv(\"filepath\\\\filename.csv\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
